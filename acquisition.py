import torch
import numpy as np
from copy import deepcopy


@torch.no_grad()
def binary_entropy(p, eps=1e-9):
    """Elementwise Bernoulli entropy H(p) = -p log p - (1-p) log(1-p)"""
    p = torch.clamp(p, 0.0 + eps, 1.0 - eps)
    return -(p * torch.log(p) + (1.0 - p) * torch.log(1.0 - p))


@torch.no_grad()
def pig_pairwise(flow, x, candidates, s):
    """
    Preference-Information-Gain (PIG) for k=2 (pairwise) comparisons.
    For each candidate x, sample a second point x' from a proposal (Uniform on domain) 
    and compute the expected entropy reduction under the current PrefFlow and temperature s.

    Args:
    - flow: trained PrefFlow model
    - x: (1,d) tensor - the fixed first alternative
    - candidates: (N,d) tensor - N possible opponents
    - s: temperature parameter for softmax likelihood

    Returns:
    - entropy: (N,) numpy array - expected entropy reduction for each opponent
    """
    logf_x, _ = flow.f(x)  # (1,)
    logf_c, _ = flow.f(candidates)  # (N,)

    # Probability x beats c under current flow (logistic on logf difference)
    delta = (logf_x - logf_c) / s
    p_win = torch.sigmoid(delta).flatten()  # (N,)

    # Bernoulli entropy
    entropy = binary_entropy(p_win)
    return entropy.cpu().numpy()


@torch.no_grad()
def eeig_pairwise(flow, ppost, x, candidates, s=1.0):
    """
    Edge-Entropy Information Gain (EEIG) approximation for pairwise comparisons.

    This computes expected reduction in the sum of edge entropies if we were to
    observe a single new preference outcome generated by comparing (anchor, opponent).

    This implementation matches the current "feature-only" approximation:
    we pretend a one-point update is "adding (x, dummy_y)" separately for the
    anchor and the opponent, then mix by the model's win prob.

    Args:
    - flow: PrefFlow model (used to compute preference likelihood)
    - ppost: ParentPosterior instance (Bayesian model over causal graphs)
    - x: (1, d) tensor — the first fixed alternative
    - candidates: (N, d) tensor — N possible opponents
    - s: temperature for preference model

    Returns:
    - entropy: (N,) numpy array - expected entropy reduction for each candidate
    """
    logf_x, _ = flow.f(x)  # (1,)
    logf_c, _ = flow.f(candidates)  # (N,)

    # Prior entropy
    H_prior = binary_entropy(ppost.edge_posterior()).sum()

    # Probability anchor wins under flow
    p_win = torch.sigmoid((logf_x - logf_c) / s).flatten()  # (N,)

    # Dummy y to simulate one-point updates
    # This is a placeholder since we don't have actual y values here
    dummy_y = torch.tensor([0.0], device=x.device, dtype=torch.float64)

    ig_vals = []
    for i in range(candidates.shape[0]):
        # Simulate anchor wins
        ppost1 = deepcopy(ppost)
        ppost1.add_datapoint(x, dummy_y)
        ppost1.update_posterior()
        H1 = binary_entropy(ppost1.edge_posterior()).sum()

        # Simulate candidate wins
        ppost2 = deepcopy(ppost)
        ppost2.add_datapoint(candidates[i:i+1], dummy_y)
        ppost2.update_posterior()
        H2 = binary_entropy(ppost2.edge_posterior()).sum()

        # Expected entropy reduction
        H_post = p_win[i] * H1 + (1 - p_win[i]) * H2
        ig_vals.append((H_prior - H_post).item())

    return np.array(ig_vals)  # (N,)